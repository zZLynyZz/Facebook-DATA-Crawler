import asyncio
import json
import csv
import os
import base64
import re
import random
from playwright.async_api import async_playwright

# --- C·∫§U H√åNH M·∫∂C ƒê·ªäNH ---
DEFAULT_TARGET_URL = "https://www.facebook.com/abcxyz" # Page m·∫∑c ƒë·ªãnh
DEFAULT_OUTPUT_FILE = 'data/posts_detail.csv'       # File ƒë·∫ßu ra
DEFAULT_MAX_POSTS = 5                                   # S·ªë b√†i m·∫∑c ƒë·ªãnh
CURRENT_PROFILE_NAME = "acc_clone_1"                    # T√™n profile chrome
SCROLL_DELAY = 3                                        # Th·ªùi gian ngh·ªâ cu·ªôn

class FacebookPostCrawler:
    def __init__(self, target_url=None, max_posts=None):
        # ∆Øu ti√™n l·∫•y tham s·ªë t·ª´ file main truy·ªÅn qua
        self.target_url = target_url if target_url else DEFAULT_TARGET_URL
        self.max_posts = max_posts if max_posts else DEFAULT_MAX_POSTS
        
        # Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n l∆∞u file v√† profile ng∆∞·ªùi d√πng
        self.output_path = os.path.join(os.getcwd(), DEFAULT_OUTPUT_FILE)
        self.user_data_dir = os.path.join(os.getcwd(), "profiles", CURRENT_PROFILE_NAME)
        
        self.post_counter = 0        # Bi·∫øn ƒë·∫øm s·ªë b√†i thu th·∫≠p ƒë∆∞·ª£c
        self.captured_fb_ids = set() # T·∫≠p h·ª£p l∆∞u ID ƒë·ªÉ ch·ªëng tr√πng b√†i
        
        # Kh·ªüi t·∫°o th∆∞ m·ª•c v√† ghi d√≤ng ti√™u ƒë·ªÅ (Header) cho file CSV
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
        self.headers = ["post_id", "user_id", "social_user", "context_content", "post_link", "post_fb_id"]
        with open(self.output_path, "w", newline="", encoding="utf-8-sig") as f:
            csv.writer(f).writerow(self.headers)
            
        print("-" * 40)
        print(f"üßπ [INIT] ƒê√£ t·∫°o file s·∫°ch: {DEFAULT_OUTPUT_FILE}")
        print(f"üéØ [TARGET] Page: {self.target_url}")
        print(f"üî¢ [LIMIT] L·∫•y t·ªëi ƒëa: {self.max_posts} b√†i")
        print("-" * 40)

    # Gi·∫£i m√£ ID t·ª´ Base64 sang d·∫°ng s·ªë (VD: Uzpf... -> 1000...)
    def extract_numeric_id(self, base64_id):
        if not base64_id: return None
        try:
            if re.match(r'^\d+$', str(base64_id)): return str(base64_id) # N·∫øu ƒë√£ l√† s·ªë
            decoded_bytes = base64.b64decode(base64_id) # Gi·∫£i m√£ base64
            decoded_str = decoded_bytes.decode('utf-8') # Chuy·ªÉn v·ªÅ chu·ªói
            match = re.search(r'(\d+)$', decoded_str)   # T√¨m d√£y s·ªë ·ªü cu·ªëi
            if match: return match.group(1)
        except: pass
        return None

    # T√¨m v√† l·∫•y n·ªôi dung vƒÉn b·∫£n c·ªßa b√†i vi·∫øt (x·ª≠ l√Ω c·∫£ c·∫•u h√¨nh c≈© v√† m·ªõi)
    def get_text_content(self, node):
        content = ""
        try: content = node['comet_sections']['content']['story']['message']['text']
        except:
            try: content = node['message']['text']
            except: pass
        return content.replace("\n", " ").strip() if content else "" # X√≥a xu·ªëng d√≤ng

    # L·∫•y ID v√† T√™n c·ªßa ng∆∞·ªùi/Page ƒëƒÉng b√†i
    def get_author_info(self, node):
        uid, name = "Unknown", "Unknown"
        try:
            actors = node['comet_sections']['context_layout']['story']['actors']
            if actors:
                uid = actors[0].get('id', 'Unknown')   # L·∫•y User ID
                name = actors[0].get('name', 'Unknown') # L·∫•y t√™n hi·ªÉn th·ªã
                return uid, name
        except: pass
        return uid, name

    # Ph√¢n lo·∫°i ƒë·ªÉ b·ªè qua c√°c b√†i chia s·∫ª (Share) ho·∫∑c Video
    def determine_post_type(self, node):
        try:
            if node['comet_sections']['content']['story']['attached_story']: return "Share"
        except: pass
        return "Status"

    # L∆∞u d·ªØ li·ªáu v√†o file CSV sau khi ƒë√£ l·ªçc ƒëi·ªÅu ki·ªán
    def process_and_save(self, node):
        if self.post_counter >= self.max_posts: return # D·ª´ng n·∫øu ƒë·ªß s·ªë l∆∞·ª£ng

        try:
            raw_id = node.get('id')
            fb_id = self.extract_numeric_id(raw_id) # L·∫•y ID b√†i vi·∫øt g·ªëc
            if not fb_id or fb_id in self.captured_fb_ids: return # B·ªè qua n·∫øu tr√πng

            user_id, social_user = self.get_author_info(node) # L·∫•y info t√°c gi·∫£
            if user_id == "Unknown": return 

            if self.determine_post_type(node) != "Status": return # Ch·ªâ l·∫•y Status/·∫¢nh

            content = self.get_text_content(node) # L·∫•y text b√†i vi·∫øt
            link = f"https://www.facebook.com/{user_id}/posts/{fb_id}" # T·∫°o link b√†i
            
            self.post_counter += 1 # TƒÉng bi·∫øn ƒë·∫øm
            internal_id = f"POST_{self.post_counter:03d}" # ID t·ª± tƒÉng (POST_001)
            
            # Ghi d√≤ng d·ªØ li·ªáu v√†o file CSV
            with open(self.output_path, "a", newline="", encoding="utf-8-sig") as f:
                csv.writer(f).writerow([internal_id, f"FB_{user_id}", social_user, content, link, fb_id])

            self.captured_fb_ids.add(fb_id) # ƒê∆∞a v√†o danh s√°ch ƒë√£ l·∫•y
            print(f"‚úÖ [{self.post_counter}/{self.max_posts}] {social_user}: {content[:30]}...")
        except Exception: pass

    # Duy·ªát ƒë·ªá quy ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c b√†i vi·∫øt ·∫©n trong g√≥i JSON c·ªßa Facebook
    def parse_graphql_response(self, data):
        if isinstance(data, dict):
            if 'timeline_list_feed_units' in data: # C·∫•u tr√∫c danh s√°ch b√†i vi·∫øt
                for edge in data['timeline_list_feed_units'].get('edges', []):
                    if 'node' in edge: self.process_and_save(edge['node'])
            elif data.get('__typename') in ['Story', 'CometStory']: # C·∫•u tr√∫c b√†i l·∫ª
                self.process_and_save(data)
            for v in data.values(): # Duy·ªát s√¢u xu·ªëng c√°c nh√°nh con
                if isinstance(v, (dict, list)): self.parse_graphql_response(v)
        elif isinstance(data, list):
            for item in data: self.parse_graphql_response(item)

    async def run(self):
        async with async_playwright() as p:
            # M·ªü tr√¨nh duy·ªát v·ªõi Profile c·ªë ƒë·ªãnh ƒë·ªÉ d√πng l·∫°i Cookie
            context = await p.chromium.launch_persistent_context(
                user_data_dir=self.user_data_dir, headless=False,
                args=["--disable-notifications"], viewport={"width": 1280, "height": 900}
            )
            page = context.pages[0]

            # L·∫Øng nghe c√°c g√≥i tin ph·∫£n h·ªìi t·ª´ Server Facebook
            async def handle_response(response):
                if "graphql" in response.url: # Ch·ªâ x·ª≠ l√Ω g√≥i tin GraphQL
                    try:
                        text = await response.text()
                        for line in text.split('\n'): # T√°ch t·ª´ng d√≤ng JSON
                            if line.strip():
                                try: self.parse_graphql_response(json.loads(line))
                                except: pass
                    except: pass

            page.on("response", handle_response) # K√≠ch ho·∫°t l·∫Øng nghe
            await page.goto(self.target_url)     # Truy c·∫≠p Page m·ª•c ti√™u
            await page.wait_for_timeout(3000)    # Ch·ªù trang t·∫£i

            last_count = 0
            while self.post_counter < self.max_posts:
                await page.keyboard.press("End") # Cu·ªôn xu·ªëng cu·ªëi trang
                await asyncio.sleep(random.uniform(SCROLL_DELAY, SCROLL_DELAY + 2))
                if self.post_counter == last_count: # N·∫øu kh√¥ng c√≥ b√†i m·ªõi
                    await asyncio.sleep(2) # Ch·ªù th√™m 2 gi√¢y
                last_count = self.post_counter
            await context.close() # ƒê√≥ng tr√¨nh duy·ªát khi xong

if __name__ == "__main__":
    asyncio.run(FacebookPostCrawler().run()) # Ch·∫°y l·∫ª module n√†y
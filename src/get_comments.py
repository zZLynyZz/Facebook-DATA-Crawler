import asyncio
import json
import csv
import os
import base64
import re
from datetime import datetime
from playwright.async_api import async_playwright

# ==============================================================================
# 1. PH·∫¶N C·∫§U H√åNH (SETTINGS)
# ==============================================================================
INPUT_POSTS_FILE = 'data/posts_detail.csv'      # File ch·ª©a danh s√°ch b√†i vi·∫øt (ƒê·∫ßu v√†o)
OUTPUT_COMMENTS_FILE = 'data/comments_detail.csv' # File ch·ª©a comment (ƒê·∫ßu ra)
CURRENT_PROFILE_NAME = "acc_clone_1"                # Profile Chrome

SCROLL_DELAY = 3      # Th·ªùi gian ngh·ªâ khi cu·ªôn (gi√¢y)
MAX_RETRIES = 3       # S·ªë l·∫ßn th·ª≠ cu·ªôn l·∫°i n·∫øu h·∫øt comment

class FacebookCommentCrawler:
    def __init__(self):
        """
        Kh·ªüi t·∫°o:
        - Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n file input/output.
        - T·∫°o file CSV ƒë·∫ßu ra ch·ª©a c√°c c·ªôt d·ªØ li·ªáu comment.
        """
        self.input_path = os.path.join(os.getcwd(), INPUT_POSTS_FILE)
        self.output_path = os.path.join(os.getcwd(), OUTPUT_COMMENTS_FILE)
        self.user_data_dir = os.path.join(os.getcwd(), "profiles", CURRENT_PROFILE_NAME)
        
        self.record_counter = 0         # Bi·∫øn ƒë·∫øm s·ªë l∆∞·ª£ng comment l·∫•y ƒë∆∞·ª£c
        self.current_post_id = ""       # L∆∞u ID b√†i vi·∫øt ƒëang x·ª≠ l√Ω hi·ªán t·∫°i (VD: POST_001)
        
        # T·∫°o th∆∞ m·ª•c ch·ª©a data n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
        
        # [C·∫¨P NH·∫¨T] ƒê·ªïi t√™n c·ªôt cu·ªëi c√πng th√†nh comment_fb_id
        self.headers = [
            'record_id',     # ID d√≤ng (REC_001)
            'source_channel',# Ngu·ªìn (Facebook)
            'post_id',       # ID b√†i vi·∫øt g·ªëc (POST_001)
            'timestamp',     # Th·ªùi gian comment
            'user_id',       # ID ng∆∞·ªùi comment
            'social_user',   # T√™n ng∆∞·ªùi comment
            'original_text', # N·ªôi dung comment
            'comment_fb_id'  # ID s·ªë c·ªßa comment (ƒê√£ s·ªë h√≥a)
        ]
        
        # T·∫°o file m·ªõi (Ghi ƒë√® - 'w')
        with open(self.output_path, "w", newline="", encoding="utf-8-sig") as f:
            csv.writer(f).writerow(self.headers)
        print(f"üßπ [INIT] ƒê√£ t·∫°o file s·∫°ch: {OUTPUT_COMMENTS_FILE}")

    # ==========================================================================
    # H√ÄM H·ªñ TR·ª¢: S·ªê H√ìA ID (M·ªöI)
    # ==========================================================================
    def extract_numeric_id(self, base64_id):
        """
        Gi·∫£i m√£ ID t·ª´ d·∫°ng Base64 (Y29tbWVudDoxMjM...) sang s·ªë (123...)
        """
        if not base64_id: return "Unknown"
        try:
            # N·∫øu ID ƒë√£ l√† s·ªë r·ªìi th√¨ tr·∫£ v·ªÅ lu√¥n
            if re.match(r'^\d+$', str(base64_id)): return str(base64_id)
            
            # Gi·∫£i m√£ Base64
            decoded_bytes = base64.b64decode(base64_id)
            decoded_str = decoded_bytes.decode('utf-8')
            
            # L·∫•y chu·ªói s·ªë cu·ªëi c√πng (Logic c·ªßa FB th∆∞·ªùng l√† Type:ID)
            match = re.search(r'(\d+)$', decoded_str)
            if match: return match.group(1)
        except: pass
        # N·∫øu kh√¥ng gi·∫£i m√£ ƒë∆∞·ª£c th√¨ tr·∫£ v·ªÅ nguy√™n g·ªëc ƒë·ªÉ debug
        return base64_id

    # ==========================================================================
    # H√ÄM ƒê·ªåC D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO
    # ==========================================================================
    def read_posts_from_csv(self):
        """ƒê·ªçc danh s√°ch b√†i vi·∫øt t·ª´ file posts_detail.csv"""
        posts = []
        if not os.path.exists(self.input_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file input: {self.input_path}")
            return posts

        with open(self.input_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('post_link'):
                    posts.append({
                        'post_id': row['post_id'],
                        'post_link': row['post_link']
                    })
        print(f"üìÇ [READ] ƒê√£ ƒë·ªçc ƒë∆∞·ª£c {len(posts)} b√†i vi·∫øt c·∫ßn l·∫•y comment.")
        return posts

    # ==========================================================================
    # H√ÄM L∆ØU D·ªÆ LI·ªÜU (SAVE)
    # ==========================================================================
    def save_to_csv(self, items):
        if not items: return
        
        with open(self.output_path, "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            for item in items:
                self.record_counter += 1
                rec_id = f"REC_{self.record_counter:03d}"
                
                raw_uid = item.get("author_id", "unknown")
                user_real_id = f"FB_{raw_uid}" if raw_uid != "unknown" else "FB_Unknown"

                writer.writerow([
                    rec_id,                     
                    'Facebook',                 
                    self.current_post_id,       
                    item.get("time"),           
                    user_real_id,               
                    item.get("name"),           
                    item.get("text"),           
                    item.get("id")  # ƒê√¢y gi·ªù l√† ID ƒë√£ ƒë∆∞·ª£c s·ªë h√≥a
                ])
                print(f"      + [{self.current_post_id}] {item.get('name')}: {item.get('text')[:30]}...")

    # ==========================================================================
    # C√ÅC H√ÄM B√ìC T√ÅCH JSON (PARSING)
    # ==========================================================================
    def find_text_recursively(self, data, depth=0):
        if depth > 5: return ""
        if isinstance(data, dict):
            if "text" in data and isinstance(data["text"], str) and len(data["text"]) > 0: return data["text"]
            for k, v in data.items():
                if k not in ["__typename", "id"]:
                    res = self.find_text_recursively(v, depth + 1)
                    if res: return res
        elif isinstance(data, list):
            for item in data:
                res = self.find_text_recursively(item, depth + 1)
                if res: return res
        return ""

    def parse_comments_json(self, data, collected_items):
        if isinstance(data, dict):
            if data.get("__typename") == "Comment":
                # 1. L·∫•y n·ªôi dung
                body = self.find_text_recursively(data.get("body", {})) or self.find_text_recursively(data)
                
                # 2. L·∫•y t√°c gi·∫£
                author_obj = data.get("author", {})
                author_name = author_obj.get("name", "Unknown")
                author_id = author_obj.get("id", "unknown")

                # 3. L·∫•y v√† x·ª≠ l√Ω ID Comment (S·ªê H√ìA)
                raw_comment_id = data.get("id", "")
                numeric_comment_id = self.extract_numeric_id(raw_comment_id)

                # 4. L·∫•y th·ªùi gian
                time_str = ""
                try:
                    ts = data.get("created_time")
                    if ts: time_str = datetime.fromtimestamp(int(ts)).strftime('%Y-%m-%d %H:%M:%S')
                except: pass

                if body:
                    collected_items.append({
                        "id": numeric_comment_id, # ƒê√£ chuy·ªÉn th√†nh s·ªë
                        "author_id": author_id,
                        "name": author_name,
                        "text": body.replace("\n", " "),
                        "time": time_str
                    })
            
            for val in data.values(): self.parse_comments_json(val, collected_items)
        
        elif isinstance(data, list):
            for item in data: self.parse_comments_json(item, collected_items)

    # ==========================================================================
    # H√ÄM CH·∫†Y CH√çNH (RUN)
    # ==========================================================================
    async def run(self):
        posts_to_crawl = self.read_posts_from_csv()
        if not posts_to_crawl: return

        async with async_playwright() as p:
            print(f"üöÄ [START] Kh·ªüi ƒë·ªông Profile: {CURRENT_PROFILE_NAME}")
            
            context = await p.chromium.launch_persistent_context(
                user_data_dir=self.user_data_dir, headless=False,
                args=["--disable-notifications"], viewport={"width": 1280, "height": 800}
            )
            page = context.pages[0]

            async def handle_response(response):
                if response.request.resource_type in ["xhr", "fetch"]:
                    try:
                        text = await response.text()
                        if text.startswith("for (;;);"): text = text[9:]
                        
                        if '"Comment"' in text or '"feedback"' in text:
                            try:
                                items = []
                                self.parse_comments_json(json.loads(text), items)
                                if items: 
                                    self.save_to_csv(items)
                            except: pass
                    except: pass
            
            page.on("response", handle_response)

            total_posts = len(posts_to_crawl)
            for i, post in enumerate(posts_to_crawl):
                self.current_post_id = post['post_id'] 
                link = post['post_link']
                
                print(f"\n[{i+1}/{total_posts}] üåê ƒêang x·ª≠ l√Ω: {self.current_post_id}")
                print(f"    üîó Link: {link}")
                
                try:
                    await page.goto(link)
                    await page.wait_for_timeout(5000)

                    # --- B∆Ø·ªöC 1: CH·ªàNH B·ªò L·ªåC ---
                    print("    ‚öôÔ∏è ƒêang ch·ªânh b·ªô l·ªçc b√¨nh lu·∫≠n...")
                    try:
                        filter_btn = page.locator("div[role='button']:has-text('Ph√π h·ª£p nh·∫•t'), div[role='button']:has-text('M·ªõi nh·∫•t'), div[role='button']:has-text('Most relevant')").first
                        if await filter_btn.is_visible():
                            await filter_btn.click()
                            await page.wait_for_timeout(2000)
                            all_opt = page.locator("div[role='menuitem']:has-text('T·∫•t c·∫£ b√¨nh lu·∫≠n'), div[role='menuitem']:has-text('All comments')").first
                            if await all_opt.is_visible():
                                await all_opt.click()
                                await page.wait_for_timeout(3000)
                            else:
                                newest_opt = page.locator("div[role='menuitem']:has-text('M·ªõi nh·∫•t'), div[role='menuitem']:has-text('Newest')").first
                                if await newest_opt.is_visible(): await newest_opt.click(); await page.wait_for_timeout(3000)
                    except: pass

                    # --- B∆Ø·ªöC 2: CU·ªòN ---
                    print(f"    üîÑ B·∫Øt ƒë·∫ßu cu·ªôn...")
                    last_count = 0
                    retry_count = 0
                    
                    while True:
                        current_count = await page.locator("div[role='article'][aria-label*='luan'], div[role='article'][aria-label*='ment']").count()
                        if current_count == 0: current_count = await page.locator("div[role='article']").count()

                        if current_count == last_count and current_count > 0:
                            retry_count += 1
                            print(f"      ‚ö†Ô∏è Ch∆∞a th·∫•y m·ªõi ({retry_count}/{MAX_RETRIES})...")
                            if retry_count >= MAX_RETRIES:
                                print(f"      üõë D·ª´ng cu·ªôn b√†i n√†y.")
                                break
                        else:
                            if current_count > last_count:
                                print(f"      ‚¨áÔ∏è T·∫£i th√™m {current_count - last_count} comment...")
                                retry_count = 0
                            last_count = current_count

                        await page.keyboard.press("End")
                        await page.wait_for_timeout(SCROLL_DELAY * 1000)

                        try:
                            view_more = page.locator("span:text('Xem th√™m b√¨nh lu·∫≠n'), span:text('View more comments')").first
                            if await view_more.is_visible(): 
                                await view_more.click()
                                await page.wait_for_timeout(2000)
                        except: pass

                except Exception as e:
                    print(f"    ‚ö†Ô∏è L·ªói: {e}")

            print(f"\nüéâ [DONE] Ho√†n th√†nh!")
            print(f"üìÇ [FILE] K·∫øt qu·∫£: {OUTPUT_COMMENTS_FILE}")

if __name__ == "__main__":
    crawler = FacebookCommentCrawler()
    asyncio.run(crawler.run())